{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDDA. Home Assignment 56"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is your last home assignment. It consists of the last two homework assignments at once. Do not worry, both are quite simple. More than that it's well equipped with comments. It should be interesting to people who are fond of Data Science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ok. Let's start.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of you have already heard about Skip-Gram Negative Sampling word embedding model. In other words it's word2vec. (*1.pdf is in repo*).\n",
    "\n",
    "For the optimisation of SGNS word embedding model SGD is usually used. Since our course is based on the fundamental concepts of algebra,  an alternative way to solve the optimization problem has to be demostrated. Just because it is the power and beauty of mathematics.\n",
    "\n",
    "Thus, the objective of this task is an optimization of SGNS formulated as implicit matrix factorization. (*2.pdf is in repo*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. Basics\n",
    "Assume we have a text corpus given as a sequence of words $\\{w_1,w_2,\\dots,w_n\\}$ where $n$ may be larger than $10^{12}$ and $w_i \\in \\mathcal{V}$ belongs to a vocabulary of words $\\mathcal{V}$. A word $c \\in \\mathcal{V}$ is called *a context* of word $w_i$ if they are found together in the text. More formally, given some measure $L$ of closeness between two words (typical choice is $L=2$), a word $c \\in \\mathcal{V}$ is called a context if $c \\in \\{w_{i-L}, \\dots, w_{i-1}, w_{i+1}, \\dots, w_{i+L} \\}$ Let $\\mathbf{w},\\mathbf{c}\\in\\mathbb{R}^d$ be the *word embeddings* of word $w$ and context $c$, respectively. Assume they are specified by the mapping  $\\Phi:\\mathcal{V}\\rightarrow\\mathbb{R}^d$, so $\\mathbf{w}=\\Phi(w)$. The ultimate goal of SGNS word embedding model is to fit a good mapping $\\Phi$.\n",
    "\n",
    "Let $\\mathcal{D}$ be a multiset of all word-contexts pairs observed in the corpus. In the SGNS model, the probability that word-context pair $(w,c)$ is observed in the corpus is modeled as the following distribution:\n",
    "\n",
    "$$\n",
    "P(\\#(w,c)\\neq 0|w,c) = \\sigma(\\mathbf{w}^\\top \\mathbf{c}) = \\frac{1}{1 + \\exp(-\\mathbf{w}^\\top \\mathbf{c})},\n",
    "$$\n",
    "\n",
    "where $\\#(w,c)$ is the number of times the pair $(w,c)$ appears in $\\mathcal{D}$ and $\\mathbf{w}^\\top\\mathbf{c}$ is the scalar product of vectors $\\mathbf{w}$ and $\\mathbf{c}$. Two important quantities which we will also use further are the number of times the word $w$ and the context $c$ appear in $\\mathcal{D}$, which can be computed as\n",
    "\n",
    "$$\n",
    "\\#(w) = \\sum_{c\\in\\mathcal{V}} \\#(w,c), \\quad \\#(c) = \\sum_{w\\in\\mathcal{V}} \\#(w,c).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. Optimization objective\n",
    "\n",
    "Vanilla word embedding models are trained by maximizing log-likelihood of observed word-context pairs, namely\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{w \\in \\mathcal{V}} \\sum_{c \\in \\mathcal{V}} \\#(w,c) \\log \\sigma(\\mathbf{w}^\\top\\mathbf{c}) \\rightarrow \\max_{\\mathbf{w},\\mathbf{c} \\in \\mathbb{R}^d}.\n",
    "$$\n",
    "\n",
    "Skip-Gram Negative Sampling approach modifies the objective by additionally minimizing the log-likelihood of random word-context pairs, so called *negative samples*. This idea incorporates some useful linguistic information that some number ($k$, usually $k=5$) of word-context pairs *are not* found together in the corpus which usually results in word embeddings of higher quality. The resulting optimization problem is\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{w \\in \\mathcal{V}} \\sum_{c \\in \\mathcal{V}} \\left( \\#(w,c) \\log \\sigma(\\mathbf{w}^\\top\\mathbf{c}) + k \\cdot \\mathbb{E}_{c'\\sim P_\\mathcal{D}} \\log \\sigma (-\\mathbf{w}^\\top\\mathbf{c}) \\right) \\rightarrow \\max_{\\mathbf{w},\\mathbf{c} \\in \\mathbb{R}^d},\n",
    "$$\n",
    "\n",
    "where $P_\\mathcal{D}(c)=\\frac{\\#(c)}{|\\mathcal{D}|}$ is a probability distribution over word contexts from which negative samples are drawn.\n",
    "\n",
    "*2.pdf* showed that this objective can be equivalently written as\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{w \\in \\mathcal{V}} \\sum_{c \\in \\mathcal{V}} f(w,c) = \\sum_{w \\in \\mathcal{V}} \\sum_{c \\in \\mathcal{V}} \\left( \\#(w,c) \\log \\sigma(\\mathbf{w}^\\top\\mathbf{c}) + \\frac{k\\cdot\\#(w)\\cdot\\#(c)}{|\\mathcal{D}|} \\log \\sigma (-\\mathbf{w}^\\top\\mathbf{c}) \\right) \\rightarrow \\max_{\\mathbf{w},\\mathbf{c} \\in \\mathbb{R}^d},\n",
    "$$\n",
    "\n",
    "A crucial observation is that this loss function depends only on the scalar product $\\mathbf{w}^\\top\\mathbf{c}$ but not on embedding $\\mathbf{w}$ and $\\mathbf{c}$ separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3. Matrix factorization problem statement\n",
    "\n",
    "Let $|\\mathcal{V}|=m$, $W \\in \\mathbb{R}^{m\\times d}$ and $C \\in \\mathbb{R}^{m\\times d}$ be matrices, where each row $\\mathbf{w}\\in\\mathbb{R}^d$ of matrix $W$ is the word embedding of the corresponding word $w$ and each row $\\mathbf{c}\\in\\mathbb{R}^d$ of matrix $C$ is the context embedding of the corresponding context $c$. SGNS embeds both words and their contexts into a low-dimensional space $\\mathbb{R}^d$, resulting in the word and context matrices $W$ and $C$. The rows of matrix $W$ are typically used in NLP tasks (such as computing word similarities) while $C$ is ignored. It is nonetheless instructive to consider the product $W^\\top C = M$. Viewed this way, SGNS can be described as factorizing an implicit matrix $M$ of dimensions $m \\times m$ into two smaller matrices.\n",
    "\n",
    "Which matrix is being factorized? A matrix entry $M_{wc}$ corresponds to the dot product $\\mathbf{w}^\\top\\mathbf{c}$ . Thus, SGNS is factorizing a matrix in which each row corresponds to a word $w \\in \\mathcal{V}$ , each column corresponds to a context $c \\in \\mathcal{V}$, and each cell contains a quantity $f(w,c)$ reflecting the strength of association between that particular word-context pair. Such word-context association matrices are very common in the NLP and word-similarity literature. That said, the objective of SGNS does not explicitly state what this association metric is. What can we say about the association function $f(w,c)$? In other words, which matrix is SGNS factorizing? Below you will find the answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home assignment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This part is pure theoretical. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve SGNS optimization problem with respect to the $\\mathbf{w}^\\top\\mathbf{c}$ and show that the matrix being factorized is\n",
    "\n",
    "$$\n",
    "M_{wc} = \\mathbf{w}^\\top\\mathbf{c} = \\log \\left( \\frac{\\#(w,c) \\cdot |\\mathcal{D}|}{k\\cdot\\#(w)\\cdot\\#(c)} \\right)\n",
    "$$\n",
    "\n",
    "**Hint:** Denote $x=\\mathbf{w}^\\top\\mathbf{c}$, rewrite SGNG optimization problem in terms of $x$ and solve it.\n",
    "                                                       \n",
    "**Note:** This matrix is called Shifted Pointwise Mutual Information (SPMI) matrix, as its elements can be written as\n",
    "\n",
    "$$\n",
    "\\text{SPMI}(w,c) = M_{wc} = \\mathbf{w}^\\top\\mathbf{c} = \\text{PMI}(w,c) - \\log k\n",
    "$$\n",
    "\n",
    "and $\\text{PMI}(w,c) = \\log \\left( \\frac{\\#(w,c) \\cdot |\\mathcal{D}|}{\\#(w)\\cdot\\#(c)} \\right)$ is the well-known *pointwise mutual information* of $(w,c)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution (proof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local objective for a specific (w, c) pair:\n",
    "$$\n",
    "\\mathcal{l}(w,c) = \\#(w,c)\\log\\sigma(\\mathbf{w}^\\top\\mathbf{c}) + k\\cdot\\#(w)\\cdot\\frac{\\#(c)}{|\\mathcal{D}|}\\log\\sigma(-\\mathbf{w}^\\top\\mathbf{c})\n",
    "$$\n",
    "To optimize the objective, we define $x=\\mathbf{w}^\\top\\mathbf{c}$ and find its partial derivative with respect to $x:$\n",
    "$$\\frac{\\partial \\mathcal{l}}{\\partial x} = \\#(w,c)\\sigma(-x) + k\\cdot\\#(w)\\cdot\\frac{\\#(c)}{|\\mathcal{D}|}\\log\\sigma(x)$$\n",
    "We compare the derivative to zero, and after some simplification, arrive at:\n",
    "$$e^{2x} - \\left( \\frac{\\#(w,c) \\cdot |\\mathcal{D}|}{k\\cdot\\#(w)\\cdot\\#(c)} - 1 \\right)e^x - \\left( \\frac{\\#(w,c) \\cdot |\\mathcal{D}|}{k\\cdot\\#(w)\\cdot\\#(c)} \\right) = 0$$\n",
    "If we define $y = e^x,$ this equation becomes a quadratic equation of $y$, which has two solutions,\n",
    "$y = âˆ’1$ (which is invalid given the definition of $y$) and:\n",
    "$$y = \\frac{\\#(w,c) \\cdot |\\mathcal{D}|}{k\\cdot\\#(w)\\cdot\\#(c)}$$\n",
    "Substituting $y$ with $e^x$ and $x$ with $\\mathbf{w}^\\top\\mathbf{c}$ reveals:\n",
    "$$\\mathbf{w}^\\top\\mathbf{c} = \\log \\left( \\frac{\\#(w,c) \\cdot |\\mathcal{D}|}{k\\cdot\\#(w)\\cdot\\#(c)} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home assignment 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This part is pure practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse.linalg import svds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have prepared the dataset (*dataset.txt is in repo*) for you. It contains compressed Wikipedia articles. Let's have a look at data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "file = open(\"./dataset.txt\", \"r\")\n",
    "doclist = [line for line in file]\n",
    "docstr = ''.join(doclist)\n",
    "sentences = re.split(r'[.!?]', docstr)\n",
    "sentences = [sentence.split() for sentence in sentences if len(sentence) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['achilles', 'wrath', 'is', 'terrible', 'and', 'he', 'slays', 'many', 'trojan', 'warriors', 'and', 'allies', 'including', 'priam', 's', 'son', 'lycaon', 'whom', 'achilles', 'had', 'previously', 'captured', 'and', 'sold', 'into', 'slavery', 'but', 'who', 'had', 'been', 'returned', 'to', 'troy']\n"
     ]
    }
   ],
   "source": [
    "print (sentences[1249])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the word vocabulary from the obtained sentences which enumerates words which occur more than $r=200$ times in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def create_vocabulary(sentences, r=200):\n",
    "    vocabulary = {}\n",
    "    words = []\n",
    "    for item in sentences:\n",
    "        words.extend(item)\n",
    "    count = Counter(words)\n",
    "    i = 0\n",
    "    for item in count:\n",
    "        if count[str(item)]>=r and item not in vocabulary:\n",
    "            vocabulary[item] = i \n",
    "            i += 1\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = create_vocabulary(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scan the text corpus with sliding window of size $5$ and step $1$ (which corresponds to $L$=2) and construct co-occurrence word-context matrix $D$ with elements $D_{wc}=\\#(w,c)$. Please, ignore words which occur less than $r=200$ times, but include them into the sliding window. Please, see the graphical illustration of the procedure described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus_matrix(sentences, vocabulary):\n",
    "    n = len(vocabulary)\n",
    "    corpus_matrix = np.zeros((n,n))\n",
    "    for i in range(len(sentences)):\n",
    "        for  j in range(len(sentences[i])):\n",
    "            if sentences[i][j] in vocabulary:\n",
    "                if max(0,j-2) == j-2:\n",
    "                    for word in sentences[i][j-2:j]:\n",
    "                        if word in vocabulary:\n",
    "                            corpus_matrix[vocabulary[sentences[i][j]], vocabulary[word]] += 1\n",
    "                        else:\n",
    "                            continue\n",
    "                else:\n",
    "                    for word in sentences[i][0:j]:\n",
    "                        if word in vocabulary:\n",
    "                                corpus_matrix[vocabulary[sentences[i][j]], vocabulary[word]] += 1\n",
    "                        else:\n",
    "                            continue\n",
    "                if min(j+3, len(sentences[i])) == j+3:\n",
    "                    for word in sentences[i][j+1:j+3]:\n",
    "                        if word in vocabulary:\n",
    "                                corpus_matrix[vocabulary[sentences[i][j]], vocabulary[word]] += 1\n",
    "                        else:\n",
    "                            continue\n",
    "                else:\n",
    "                    for word in sentences[i][j+1:len(sentences[i])]:\n",
    "                        if word in vocabulary:\n",
    "                                corpus_matrix[vocabulary[sentences[i][j]], vocabulary[word]] += 1\n",
    "                        else:\n",
    "                            continue\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "    return corpus_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = create_corpus_matrix(sentences, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find good word embeddings, *2.pdf* proposed to find rank-$d$ SVD of Shifted Positive Pointwise Mutual Information (SPPMI) matrix\n",
    "\n",
    "$$\n",
    "U \\Sigma V^\\top \\approx \\text{SPPMI},\n",
    "$$\n",
    "\n",
    "where $\\text{SPPMI}(w, c) = \\max\\left(\\text{SPMI}(w, c), 0 \\right)$ and $\\text{SPMI}(w, c)$ is the element of the matrix $\\text{SPPMI}$ at position $(w, c)$.\n",
    "Then use $W=U\\sqrt{\\Sigma}$ as word embedding matrix. Your task is to reproduce their results. Write function constructs $\\text{SPPMI}$ matrix, computes its SVD and produces word-vectors matrix $W$. Pay attention that $\\text{SPPMI}$ matrix is **sparse**!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def compute_embeddings(D, k, d=200):\n",
    "    rows = np.sum(D, axis = 1)[:, np.newaxis]\n",
    "    clmn = np.sum(D, axis = 0)\n",
    "    M = np.maximum(np.log(np.sum(D)/k*((D/rows)/clmn) ),0)\n",
    "    U,S,V = svds(csr_matrix(M),d)\n",
    "    embedding_matrix = U @ np.diag(np.sqrt(S))\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "k = 5 # negative sampling parameter\n",
    "W = compute_embeddings(D, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write class **WordVectors** using provided template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordVectors:\n",
    "    \n",
    "    def __init__(self, vocabulary, embedding_matrix):\n",
    "        self.vocab = vocabulary\n",
    "        self.W = embedding_matrix\n",
    "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        \n",
    "    def word_vector(self, word):\n",
    "        \"\"\" \n",
    "        Takes word and returns its word vector.\n",
    "        \"\"\"\n",
    "        word_vector = self.embedding_matrix[self.vocab[word]]\n",
    "        return word_vector\n",
    "    \n",
    "    def nearest_words(self, word, top_n=10):\n",
    "        \"\"\" \n",
    "        Takes word from the vocabulary and returns its top_n\n",
    "        nearest neighbors in terms of cosine similarity.\n",
    "        \"\"\"\n",
    "        cos = np.zeros(len(self.vocab))\n",
    "        W_norm = self.W[self.vocab[word]] / np.linalg.norm(self.W[self.vocab[word]])\n",
    "        for i in range(len(self.vocab)):\n",
    "            cos[i] = np.dot(W_norm,self.W[i]) / np.linalg.norm(self.W[i])\n",
    "        neighbors = [(self.inv_vocab[i], round(cos[i],3)) for i in np.argsort(cos)[-(top_n+1):-1]][::-1]\n",
    "        return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WordVectors(vocab, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('communism', 0.792),\n",
       " ('anarcho', 0.787),\n",
       " ('capitalism', 0.784),\n",
       " ('socialism', 0.752),\n",
       " ('liberalism', 0.727),\n",
       " ('criticisms', 0.705),\n",
       " ('capitalist', 0.662),\n",
       " ('fascism', 0.565),\n",
       " ('anarchist', 0.527),\n",
       " ('marxist', 0.518)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.nearest_words(\"anarchism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hop', 0.828),\n",
       " ('hip', 0.817),\n",
       " ('funk', 0.758),\n",
       " ('rock', 0.737),\n",
       " ('punk', 0.706),\n",
       " ('music', 0.676),\n",
       " ('pop', 0.666),\n",
       " ('scene', 0.659),\n",
       " ('band', 0.657),\n",
       " ('jazz', 0.625)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.nearest_words(\"rap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ukraine', 0.671),\n",
       " ('russia', 0.629),\n",
       " ('poland', 0.55),\n",
       " ('belarus', 0.538),\n",
       " ('yugoslavia', 0.538),\n",
       " ('romania', 0.517),\n",
       " ('serbia', 0.507),\n",
       " ('austria', 0.5),\n",
       " ('hungary', 0.466),\n",
       " ('bulgaria', 0.43)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.nearest_words(\"ussr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate top 10 nearest neighbours with the corresponding cosine similarities for the words {numerical, linear, algebra}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('computation', 0.547),\n",
       " ('mathematical', 0.532),\n",
       " ('calculations', 0.499),\n",
       " ('polynomial', 0.485),\n",
       " ('calculation', 0.473),\n",
       " ('practical', 0.46),\n",
       " ('statistical', 0.456),\n",
       " ('symbolic', 0.455),\n",
       " ('geometric', 0.441),\n",
       " ('simplest', 0.438)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.nearest_words(\"numerical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('differential', 0.759),\n",
       " ('equations', 0.724),\n",
       " ('equation', 0.682),\n",
       " ('continuous', 0.674),\n",
       " ('multiplication', 0.674),\n",
       " ('integral', 0.672),\n",
       " ('algebraic', 0.667),\n",
       " ('vector', 0.654),\n",
       " ('algebra', 0.63),\n",
       " ('inverse', 0.622)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.nearest_words(\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('geometry', 0.795),\n",
       " ('calculus', 0.73),\n",
       " ('algebraic', 0.716),\n",
       " ('differential', 0.687),\n",
       " ('equations', 0.665),\n",
       " ('equation', 0.648),\n",
       " ('theorem', 0.647),\n",
       " ('topology', 0.634),\n",
       " ('linear', 0.63),\n",
       " ('integral', 0.618)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.nearest_words(\"algebra\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
